{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c27eea12",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:#000; font-family: 'Arial'; font-size: 2em;\">BIG DATA</span>\n",
    "\n",
    "<span style=\"color:#f00; font-family: 'Arial'; font-size: 1.5em;\">Unit 7: Apache Spark: Cluster Deployment and Performance Optimization </span>\n",
    "\n",
    "<span style=\"color:#300; font-family: 'Arial'; font-size: 1.5em;\"></span>\n",
    "<h4 style=\"color:darkblue\"> Universidad de Deusto</h4>\n",
    "\n",
    "<span style=\"color:#300; font-family: 'Arial'; font-size: 1em;\">m.varo@deusto.es</span>\n",
    "\n",
    "<h5 style=\"color:black\">  11 de abril de 2025 - Donostia </h5>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c3be07",
   "metadata": {},
   "source": [
    "### 1. Spark Memory Management\n",
    "\n",
    "It's important to note that Spark does **not** automatically cache input data in memory. A common misconception is that Spark cannot be used effectively unless the input data fits entirely in memory. This is **not true**. Spark is capable of processing terabytes of data even on clusters with limited memory—for example, a cluster with only 5 GB of total memory.\n",
    "\n",
    "Deciding what data to cache, and when to cache it during a data processing pipeline, is the responsibility of the application developer. In fact, if a Spark application only makes a single pass over the data, caching may not be necessary at all.\n",
    "\n",
    "Another reason Spark outperforms Hadoop MapReduce is its advanced job execution engine. Like MapReduce, Spark represents jobs as Directed Acyclic Graphs (DAGs) of stages, but it processes these DAGs more efficiently, enabling better performance and reduced execution time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c2faf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Step 1: Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CacheExample\") \\\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41ff3ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load a sample dataset\n",
    "df = spark.range(0, 1000000)  # Creates a DataFrame with numbers from 0 to 999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b126e519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Cache the DataFrame in memory\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "738eaa76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Trigger an action to materialize the cache\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8304f2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the DataFrame cached? True\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Check if it's cached\n",
    "print(f\"Is the DataFrame cached? {df.is_cached}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a6739a",
   "metadata": {},
   "source": [
    "- After running df.cache() and an action like df.count(), Spark will cache the data in memory.\n",
    "\n",
    "- You can open your Spark UI in your browser at http://localhost:4040 — there you’ll see a tab called \"Storage\", which shows cached RDDs/DataFrames and their memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b41b6e8",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. Optimization in Spark vs. Hadoop MapReduce\n",
    "\n",
    "Hadoop MapReduce creates a Directed Acyclic Graph (DAG) with exactly two predefined stages—Map and Reduce—for every job. A complex data processing algorithm in MapReduce may require multiple jobs to be executed sequentially, which prevents any optimization across jobs.\n",
    "\n",
    "In contrast, Spark offers greater flexibility. It does not force the developer to break a complex algorithm into multiple jobs. A Spark DAG can contain any number of stages, allowing both simple jobs with just one stage and more complex jobs with several stages. This ability enables Spark to perform optimizations that are not possible in MapReduce.\n",
    "\n",
    "Spark executes a multi-stage complex job in a single run, leveraging the knowledge of all stages to optimize the execution. For example, it can minimize disk I/O and data shuffling, which involves transferring data across the network and significantly increasing application execution time. By reducing these costly operations, Spark can improve overall job performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24363ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, avg, sum\n",
    "\n",
    "# Example: Complex Optimization in Spark vs. Hadoop MapReduce\n",
    "\n",
    "# Extend the existing data with additional columns\n",
    "data = [(1, 'Alice', 1000, 'HR'), (2, 'Bob', 2000, 'IT'), (3, 'Charlie', 3000, 'Finance')]\n",
    "columns = ['ID', 'Name', 'Salary', 'Department']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Transformation 1: Add a new column with salary categories\n",
    "df = df.withColumn(\n",
    "    'Salary Category',\n",
    "    when(col('Salary') < 1500, 'Low')\n",
    "    .when((col('Salary') >= 1500) & (col('Salary') < 2500), 'Medium')\n",
    "    .otherwise('High')\n",
    ")\n",
    "\n",
    "# Transformation 2: Group by Department and Salary Category, and calculate aggregates\n",
    "aggregated_df = df.groupBy('Department', 'Salary Category').agg(\n",
    "    avg('Salary').alias('Average Salary'),\n",
    "    sum('Salary').alias('Total Salary')\n",
    ")\n",
    "\n",
    "# Explanation:\n",
    "# - Spark optimizes the execution by combining transformations into a single DAG.\n",
    "# - It minimizes intermediate data writes to disk and reduces data shuffling.\n",
    "# - This approach is significantly more efficient than Hadoop MapReduce, which would require multiple jobs for similar operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27e5e999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+--------------+------------+\n",
      "|Department|Salary Category|Average Salary|Total Salary|\n",
      "+----------+---------------+--------------+------------+\n",
      "|        HR|            Low|        1000.0|        1000|\n",
      "|        IT|         Medium|        2000.0|        2000|\n",
      "|   Finance|           High|        3000.0|        3000|\n",
      "+----------+---------------+--------------+------------+\n",
      "\n",
      "Time taken for Spark operation: 0.30 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Measure the time taken for the Spark operation\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform the Spark operation (already defined in the notebook)\n",
    "aggregated_df.show()\n",
    "\n",
    "end_time = time.time()\n",
    "spark_execution_time = end_time - start_time\n",
    "print(f\"Time taken for Spark operation: {spark_execution_time:.2f} seconds\")\n",
    "\n",
    "# Note: To compare with MapReduce, you would need to implement the same logic using a MapReduce framework,\n",
    "# which is outside the scope of this notebook since it requires a Hadoop setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde9fff7",
   "metadata": {},
   "source": [
    "---\n",
    "### 4. Scalability in Spark\n",
    "\n",
    "Spark is highly scalable, allowing you to increase the data processing capacity of a cluster simply by adding more nodes. You can start with a small cluster and, as your dataset grows, scale your infrastructure by adding more computing resources. This flexibility makes Spark an economical choice for handling growing datasets.\n",
    "\n",
    "One of Spark's key features is that it automatically handles scaling without requiring any changes to the application code. When you add nodes to a Spark cluster, the application can take advantage of the additional resources without any code modifications, making it easy to scale as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22646b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the transformed DataFrame: 100000000\n"
     ]
    }
   ],
   "source": [
    "# Example: Demonstrating Spark's scalability by processing a large dataset\n",
    "\n",
    "# Generate a large dataset with 100 million rows\n",
    "large_df = spark.range(0, 100000000)\n",
    "\n",
    "# Perform a simple transformation and action to demonstrate scalability\n",
    "# Transformation: Add a new column with values doubled\n",
    "transformed_df = large_df.withColumn(\"Doubled Value\", col(\"id\") * 2)\n",
    "\n",
    "# Action: Count the number of rows in the transformed DataFrame\n",
    "row_count = transformed_df.count()\n",
    "\n",
    "print(f\"Number of rows in the transformed DataFrame: {row_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690465cf",
   "metadata": {},
   "source": [
    "Spark itself does not generate nodes; instead, it utilizes the nodes provided by the cluster manager. The process of adding more nodes to a Spark cluster depends on the cluster manager being used (e.g., Standalone, YARN, or Mesos). Here's how it works:\n",
    "\n",
    "1. **Cluster Manager**: Spark relies on a cluster manager to allocate resources (CPU, memory, etc.) for its applications. The cluster manager is responsible for managing the worker nodes in the cluster.\n",
    "\n",
    "2. **Adding Nodes**:\n",
    "    - **Standalone Mode**: In Spark's standalone mode, you can add more worker nodes by starting additional worker processes on new machines and connecting them to the master node.\n",
    "    - **YARN**: In a Hadoop YARN cluster, you can add more nodes by adding new machines to the Hadoop cluster and configuring them as YARN NodeManagers.\n",
    "    - **Mesos**: In a Mesos cluster, you can add nodes by adding new machines to the Mesos cluster and configuring them as Mesos agents.\n",
    "\n",
    "3. **Dynamic Allocation**: Spark supports dynamic resource allocation, which allows it to request additional executors (processes running on worker nodes) during runtime if the workload increases. This feature is particularly useful in shared clusters where resources are allocated on demand.\n",
    "\n",
    "4. **Scaling**: When you add nodes to the cluster, the cluster manager makes these resources available to Spark. Spark can then distribute tasks across the additional nodes, increasing parallelism and improving performance.\n",
    "\n",
    "In summary, Spark does not generate nodes itself; it depends on the cluster manager to provide and manage the nodes. To scale a Spark cluster, you need to add nodes to the underlying cluster infrastructure managed by the cluster manager."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72417a1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 5. Fault Tolerance in Spark\n",
    "\n",
    "Spark is designed to be fault-tolerant, ensuring reliable execution even in the face of hardware failures. In a cluster with hundreds of nodes, the probability of a node failure on any given day is significant—whether due to a hard disk crash or other hardware issues. However, Spark automatically handles the failure of a node in the cluster, ensuring that the application continues running.\n",
    "\n",
    "While the failure of a node may cause some performance degradation, it will not cause the application to crash. This built-in fault tolerance means that application developers do not need to explicitly handle node failures in their code, simplifying the application development process and increasing reliability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459a75f3",
   "metadata": {},
   "source": [
    "---\n",
    "### 6. Iterative Algorithms in Spark\n",
    "\n",
    "Iterative algorithms are data processing algorithms that repeatedly iterate over the same data. Applications such as machine learning and graph processing commonly use iterative algorithms, running tens or even hundreds of iterations over the same dataset. Spark is particularly well-suited for these types of applications.\n",
    "\n",
    "The reason iterative algorithms run efficiently on Spark is its in-memory computing capabilities. Spark allows applications to cache data in memory, so even if an iterative algorithm performs 100 iterations, it only needs to read the data from disk during the first iteration. Subsequent iterations can read the data from memory, which is typically **100 times faster** than reading from disk. This dramatically speeds up the execution of these applications, often resulting in orders of magnitude improvements in performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a7dd93",
   "metadata": {},
   "source": [
    "---\n",
    "### 7. Interactive Data Analysis with Spark\n",
    "\n",
    "Interactive data analysis involves exploring a dataset in real-time, allowing for quick insights before running long and resource-intensive batch processing jobs. For instance, before executing a time-consuming job that might run for hours, a data analyst might perform summary analysis on a large dataset. Similarly, business analysts often require the ability to interactively analyze data using BI or visualization tools, running multiple queries on the same data. Spark is an ideal platform for such interactive analysis of large datasets.\n",
    "\n",
    "The key advantage of Spark in interactive analysis is its **in-memory computing capabilities**. When an application caches the data to be interactively analyzed, the first query will read data from disk, but subsequent queries will access the cached data in memory. Since reading from memory is orders of magnitude faster than reading from disk, Spark can dramatically reduce query execution time. A query that would normally take over an hour when reading from disk can often be completed in just a few seconds when the data is cached in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0bce90",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 8. High-level architecture\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"spark_diagram1.png\" width=\"500\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849d34d8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Key Components in Spark\n",
    "\n",
    "- **Workers**: A worker provides CPU, memory, and storage resources to a Spark application. Workers run Spark applications as distributed processes across a cluster of nodes, enabling parallel computation.\n",
    "\n",
    "- **Cluster Managers**: Spark uses a cluster manager to acquire and manage cluster resources for executing jobs. A cluster manager, as the name implies, is responsible for managing computing resources across a cluster of worker nodes. It provides low-level scheduling of cluster resources and enables multiple applications to share resources, allowing them to run on the same worker nodes. Spark supports three cluster managers:\n",
    "  - **Standalone**: Spark's native cluster manager.\n",
    "  - **Mesos**: A general-purpose cluster manager.\n",
    "  - **YARN**: The Hadoop cluster manager.\n",
    "\n",
    "  Mesos and YARN allow Spark to run alongside Hadoop applications on the same worker nodes.\n",
    "\n",
    "- **Driver Programs**: A driver program is an application that uses Spark as a library to process data. The driver provides the data processing code that Spark executes on the worker nodes. It can launch one or more jobs on a Spark cluster.\n",
    "\n",
    "- **Executors**: An executor is a JVM (Java Virtual Machine) process created by Spark on each worker node for an application. It executes application code concurrently in multiple threads and can also cache data in memory or on disk. The lifespan of an executor is tied to the lifespan of the application. When the Spark application terminates, all executors associated with it are also terminated.\n",
    "\n",
    "- **Tasks**: A task is the smallest unit of work that Spark sends to an executor. It is executed by a thread in an executor on a worker node. Each task performs computations to either return a result to the driver program or partition its output for shuffling. Spark creates one task per data partition, and an executor runs multiple tasks concurrently. The level of parallelism is determined by the number of partitions—more partitions lead to more tasks running in parallel.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965c5e39",
   "metadata": {},
   "source": [
    "---\n",
    "### 9. Application execution\n",
    "\n",
    "This section briefly describes how data processing code is executed on a Spark cluster.\n",
    "\n",
    "#### Terminology\n",
    "\n",
    "Before we dive into the execution details, let's define some key terms:\n",
    "\n",
    "- **Shuffle**: A shuffle is the process of redistributing data across the nodes of a cluster. It is an expensive operation because it involves moving data over the network. However, a shuffle does not randomly distribute data; instead, it groups data elements into buckets based on specific criteria. Each bucket forms a new partition.\n",
    "  \n",
    "- **Job**: A job is a set of computations that Spark performs to return results to the driver program. Essentially, it represents the execution of a data processing algorithm on a Spark cluster. An application can launch multiple jobs. The specifics of how a job is executed will be covered later in this chapter.\n",
    "  \n",
    "- **Stage**: A stage is a collection of tasks. Spark splits a job into a Directed Acyclic Graph (DAG) of stages, and stages may depend on one another. For example, a job could be divided into two stages—stage 0 and stage 1—where stage 1 cannot begin until stage 0 has completed. Spark groups\n",
    "\n",
    "---\n",
    " \n",
    " Reference: 'Big Data Analytics with Spark, Mohammed Guller'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7962a0a0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
