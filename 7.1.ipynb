{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c27eea12",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 2em;\">BIG DATA</span>\n",
    "\n",
    "<span style=\"color:#f00; font-family: 'Bebas Neue'; font-size: 1.5em;\">Unit 7: Cluster Management </span>\n",
    "\n",
    "<span style=\"color:#300; font-family: 'Bebas Neue'; font-size: 1.5em;\"></span>\n",
    "<h4 style=\"color:darkblue\"> Universidad de Deusto</h4>\n",
    "\n",
    "<span style=\"color:#300; font-family: 'Bebas Neue'; font-size: 1em;\">m.varo@deusto.es</span>\n",
    "\n",
    "<h5 style=\"color:black\">  11 de abril de 2025 - Donostia </h5>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce41dc29",
   "metadata": {},
   "source": [
    "\n",
    "1. **Deploying a Multi-Node Cluster**: Set up a Spark Standalone or YARN cluster (depending on resource availability).\n",
    "2. **Job Submission**: Run Spark applications on the configured cluster.\n",
    "3. **Monitoring Tools**: Utilize Spark UI and Ganglia to monitor and analyze job performance.\n",
    "4. **Fault Tolerance Simulation**: Simulate failures and test Spark's job recovery mechanisms.\n",
    "\n",
    "\n",
    "### **Optional Additions**  \n",
    "#### Code Block (example):  \n",
    "```bash\n",
    "spark-submit --master yarn --deploy-mode cluster your_app.py\n",
    "```\n",
    "\n",
    "#### Table (comparison):  \n",
    "| Cluster Type  | Pros                     | Cons                     |  \n",
    "|--------------|--------------------------|--------------------------|  \n",
    "| Standalone   | Easy setup               | Limited resource sharing |  \n",
    "| YARN         | Better resource management | More complex configuration |  \n",
    "\n",
    "#### Links:  \n",
    "- [Spark UI Guide](https://spark.apache.org/docs/latest/web-ui.html)  \n",
    "- [Ganglia Monitoring](http://ganglia.sourceforge.net/)  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c3be07",
   "metadata": {},
   "source": [
    "# Apache Spark: Cluster Deployment and Performance Optimization\n",
    "\n",
    "## 1. Introduction to Apache Spark\n",
    "Apache Spark is one of the most active and widely adopted open-source projects in the big data ecosystem...\n",
    "\n",
    "## 2. Spark Memory Management\n",
    "\n",
    "It's important to note that Spark does **not** automatically cache input data in memory. A common misconception is that Spark cannot be used effectively unless the input data fits entirely in memory. This is **not true**. Spark is capable of processing terabytes of data even on clusters with limited memoryâ€”for example, a cluster with only 100 GB of total memory.\n",
    "\n",
    "Deciding what data to cache, and when to cache it during a data processing pipeline, is the responsibility of the application developer. In fact, if a Spark application only makes a single pass over the data, caching may not be necessary at all.\n",
    "\n",
    "Another reason Spark outperforms Hadoop MapReduce is its advanced job execution engine. Like MapReduce, Spark represents jobs as Directed Acyclic Graphs (DAGs) of stages, but it processes these DAGs more efficiently, enabling better performance and reduced execution time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d50ccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Caching vs Non-Caching\n",
    "rdd = sc.textFile(\"hdfs:///bigdata/input.txt\")\n",
    "\n",
    "# Without caching\n",
    "word_counts = rdd.flatMap(lambda x: x.split()) \\\n",
    "                 .map(lambda word: (word, 1)) \\\n",
    "                 .reduceByKey(lambda a, b: a + b)\n",
    "word_counts.collect()\n",
    "\n",
    "# With caching\n",
    "rdd.cache()\n",
    "word_counts = rdd.flatMap(lambda x: x.split()) \\\n",
    "                 .map(lambda word: (word, 1)) \\\n",
    "                 .reduceByKey(lambda a, b: a + b)\n",
    "word_counts.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5820b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
